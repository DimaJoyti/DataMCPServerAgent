"""
Complete advanced RL example demonstrating all implemented features.
This example showcases distributed RL, hyperparameter optimization,
safe RL, and explainable RL.
"""

import asyncio
import os
import sys
from typing import Any

# Third-party imports
from dotenv import load_dotenv

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Use lazy imports for memory optimization
from src.utils.lazy_imports import numpy as np, langchain_anthropic
from src.utils.memory_monitor import MemoryContext, log_memory_usage
from src.agents.distributed_rl import create_distributed_rl_system
from src.agents.explainable_rl import create_explainable_rl_agent
from src.agents.modern_deep_rl import DQNAgent
from src.agents.reinforcement_learning import RewardSystem
from src.agents.safe_rl import (
    ResourceUsageConstraint,
    ResponseTimeConstraint,
    create_safe_rl_agent,
)
from src.memory.memory_persistence import MemoryDatabase
from src.optimization.hyperparameter_optimization import (
    create_rl_hyperparameter_optimizer,
)

# Access ChatAnthropic through lazy loader
ChatAnthropic = langchain_anthropic.ChatAnthropic

# Load environment variables
load_dotenv()


async def demonstrate_distributed_rl() -> None:
    """Demonstrate distributed reinforcement learning with memory optimization."""
    print("\nüåê Demonstrating Distributed RL")
    print("=" * 60)

    # Monitor memory usage
    with MemoryContext("distributed_rl_demo") as memory_ctx:
        log_memory_usage("Starting distributed RL demo")

        # Initialize components
        model = ChatAnthropic(
            model="claude-3-5-sonnet-20241022",
            api_key=os.getenv("ANTHROPIC_API_KEY"),
        )

        # Use optimized database with async operations
        from src.memory.database_optimization import OptimizedDatabase
        from src.memory.database_optimization import (
            apply_database_optimizations,
        )

        db_path = "distributed_rl_demo.db"
        OptimizedDatabase(db_path)  # Initialize optimized database

        # Apply database optimizations
        optimization_result = await apply_database_optimizations(db_path)
        indexes_created = optimization_result['indexes_created']
        print(f"‚úÖ Database optimized: {indexes_created} indexes created")

        # Create memory database with fallback
        try:
            db = MemoryDatabase(db_path)
            await db._initialize_db()  # Async initialization
        except Exception as e:
            print(f"‚ö†Ô∏è Using fallback database: {e}")
            db = None

        # Create distributed RL system with error handling
        try:
            distributed_coordinator = await create_distributed_rl_system(
                model=model,
                db=db,
                num_workers=4,
                model_type="dqn",
                state_dim=64,
                action_dim=5,
            )

            print("‚úÖ Created distributed RL system with:")
            print(f"   - {distributed_coordinator.num_workers} workers")
            print("   - Parameter server with weighted aggregation")
            print("   - DQN model architecture")
            print(f"   - Memory usage: {memory_ctx.memory_delta:.2f}MB")

            # Use bounded collections for training data
            from src.utils.bounded_collections import BoundedList
            training_results = BoundedList(
                max_size=100, eviction_strategy="fifo"
            )

            # Simulate distributed training
            print("\nüèãÔ∏è Running distributed training episodes...")

            requests = [
                "Analyze market trends for Q4 planning",
                "Create comprehensive project roadmap",
                "Optimize resource allocation strategy",
                "Develop risk mitigation framework",
            ]

            for i, request in enumerate(requests):
                print(f"\nüìù Episode {i+1}: {request}")

                episode_name = f"training_episode_{i+1}"
                with MemoryContext(episode_name) as episode_ctx:
                    result = await (
                        distributed_coordinator.train_distributed_episode(
                            request, []
                        )
                    )

                    # Store result in bounded collection
                    training_results.append(result)

                    if result["success"]:
                        print("   ‚úÖ Training successful")
                        avg_loss = result['avg_loss']
                        avg_reward = result['avg_reward']
                        successful_workers = result['successful_workers']
                        server_stats = result['server_stats']
                        update_count = server_stats.get('update_count', 0)
                        memory_delta = episode_ctx.memory_delta

                        print(f"   üìä Average loss: {avg_loss:.4f}")
                        print(f"   üèÜ Average reward: {avg_reward:.4f}")
                        print(f"   üë• Successful workers: {successful_workers}")
                        print(f"   üîÑ Server updates: {update_count}")
                        print(f"   üíæ Episode memory: {memory_delta:.2f}MB")
                    else:
                        error_msg = result.get('error', 'Unknown error')
                        print(f"   ‚ùå Training failed: {error_msg}")

            # Get distributed statistics
            stats = distributed_coordinator.get_distributed_statistics()
            print("\nüìà Distributed Training Statistics:")
            print(f"   Total episodes: {stats['aggregate']['total_episodes']}")
            print(f"   Global episodes: {stats['aggregate']['global_episodes']}")
            print(f"   Average reward: {stats['aggregate']['avg_reward']:.4f}")
            print(f"   Active workers: {stats['server']['active_workers']}")
            print(f"   Results stored: {len(training_results)}")

        except Exception as e:
            print(f"‚ö†Ô∏è Distributed RL demo failed: {e}")
            print("   Continuing with other demonstrations...")

    print(f"\nüíæ Total memory usage: {memory_ctx.memory_delta:.2f}MB")
    log_memory_usage("Completed distributed RL demo")


async def demonstrate_hyperparameter_optimization() -> None:
    """Demonstrate hyperparameter optimization with memory management."""
    print("\nüéØ Demonstrating Hyperparameter Optimization")
    print("=" * 60)

    with MemoryContext("hyperparameter_optimization"):
        log_memory_usage("Starting hyperparameter optimization")

        # Initialize components with dependency injection pattern
        from src.core.dependency_injection import get_container, ILogger
        from app.core.dependencies import configure_fastapi_services

        container = get_container()
        configure_fastapi_services(container)

        # Get logger service
        try:
            logger_service = container.resolve(ILogger)
            logger_service.info("Hyperparameter optimization starting")
        except Exception:
            print("‚ö†Ô∏è Using fallback logging")

        # Initialize components
        model = ChatAnthropic(
            model="claude-3-5-sonnet-20241022",
            api_key=os.getenv("ANTHROPIC_API_KEY"),
        )

        # Use optimized database
        try:
            db = MemoryDatabase("hyperopt_demo.db")
            await db._initialize_db()
        except Exception as e:
            print(f"‚ö†Ô∏è Database optimization failed: {e}")
            db = None

        # Define agent factory
        async def agent_factory(agent_type: str, params: dict[str, Any]):
            """Factory function to create agents with given parameters."""
            reward_system = RewardSystem(db)

            if agent_type == "dqn":
                return DQNAgent(
                    name="hyperopt_dqn",
                    model=model,
                    db=db,
                    reward_system=reward_system,
                    state_dim=64,
                    action_dim=5,
                    learning_rate=params.get("learning_rate", 1e-4),
                    epsilon=params.get("epsilon", 1.0),
                    epsilon_decay=params.get("epsilon_decay", 0.995),
                    target_update_freq=params.get("target_update_freq", 1000),
                    batch_size=params.get("batch_size", 32),
                    buffer_size=params.get("buffer_size", 10000),
                    gamma=params.get("gamma", 0.99),
                    double_dqn=params.get("double_dqn", True),
                    dueling=params.get("dueling", True),
                )

            raise ValueError(f"Unknown agent type: {agent_type}")

    # Create hyperparameter optimizer
    optimizer = await create_rl_hyperparameter_optimizer(
        model=model,
        db=db,
        agent_factory=agent_factory,
        optimization_method="bayesian",
        evaluation_episodes=5,  # Reduced for demo
    )

    print("‚úÖ Created hyperparameter optimizer with:")
    print("   - Bayesian optimization")
    print("   - 5 evaluation episodes per trial")
    print("   - DQN parameter space")

    # Run optimization
    print("\nüîç Running hyperparameter optimization...")

    try:
        results = await optimizer.optimize_agent(
            agent_type="dqn",
            n_trials=10,  # Reduced for demo
        )

        print("‚úÖ Optimization completed!")
        print(f"   Best performance: {results['best_value']:.4f}")
        print("   Best parameters:")
        for param, value in results['best_params'].items():
            print(f"     {param}: {value}")

        # Get optimization statistics
        stats = optimizer.get_best_hyperparameters("dqn")
        if stats:
            print("\nüìä Optimization Statistics:")
            print(f"   Total trials: {results['n_trials']}")
            print(f"   Best learning rate: {stats.get('learning_rate', 'N/A')}")
            print(f"   Best batch size: {stats.get('batch_size', 'N/A')}")

    except Exception as e:
        print(f"‚ö†Ô∏è Optimization demo skipped due to dependencies: {e}")


async def demonstrate_safe_rl() -> None:
    """Demonstrate safe reinforcement learning."""
    print("\nüõ°Ô∏è Demonstrating Safe RL")
    print("=" * 60)

    # Initialize components
    model = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        api_key=os.getenv("ANTHROPIC_API_KEY"),
    )
    db = MemoryDatabase("safe_rl_demo.db")
    reward_system = RewardSystem(db)

    # Create base agent
    base_agent = DQNAgent(
        name="base_dqn",
        model=model,
        db=db,
        reward_system=reward_system,
        state_dim=32,
        action_dim=4,
    )

    # Define safety constraints
    safety_constraints = [
        ResourceUsageConstraint(max_resource_usage=0.7),
        ResponseTimeConstraint(max_response_time=3.0),
    ]

    # Create safe RL agent
    safe_agent = await create_safe_rl_agent(
        model=model,
        db=db,
        base_agent=base_agent,
        safety_constraints=safety_constraints,
        safety_weight=0.6,
    )

    print("‚úÖ Created safe RL agent with:")
    print("   - Resource usage constraint (max 70%)")
    print("   - Response time constraint (max 3.0s)")
    print("   - Safety weight: 0.6")
    print("   - Constraint learning enabled")

    # Simulate safe decision making
    print("\nüîí Testing safe decision making...")

    test_scenarios = [
        {
            "description": "Normal operation",
            "context": {"complexity": "low", "priority": "normal"},
        },
        {
            "description": "High-priority urgent task",
            "context": {
                "complexity": "high",
                "priority": "urgent",
                "high_priority": True,
            },
        },
        {
            "description": "Resource-intensive operation",
            "context": {"complexity": "high", "batch_processing": True},
        },
        {
            "description": "Complex query with time pressure",
            "context": {"complex_query": True, "urgent": True},
        },
    ]

    for i, scenario in enumerate(test_scenarios):
        print(f"\nüß™ Scenario {i+1}: {scenario['description']}")

        # Generate test state
        state = np.random.randn(32).astype(np.float32)

        # Select safe action
        action, safety_info = await safe_agent.select_safe_action(
            state, scenario["context"], training=True
        )

        print(f"   üéØ Selected action: {action}")
        print(f"   üîÑ Action modified: {safety_info['action_modified']}")
        if safety_info['action_modified']:
            print(f"   ‚ö†Ô∏è Original action: {safety_info['original_action']}")

        safety_score = safety_info['safety_results']['safety_score']
        print(f"   üõ°Ô∏è Safety score: {safety_score:.3f}")

        # Simulate training with safety
        reward = np.random.uniform(-1, 1)
        next_state = np.random.randn(32).astype(np.float32)

        safety_results = safety_info['safety_results']
        training_metrics = await safe_agent.train_with_safety(
            state, action, reward, next_state, False, safety_results
        )

        safe_reward = training_metrics.get('safe_reward', 0)
        safety_penalty = training_metrics.get('safety_penalty', 0)
        print(f"   üìà Safe reward: {safe_reward:.3f}")
        print(f"   ‚ö° Safety penalty: {safety_penalty:.3f}")

    # Get safety performance
    performance = safe_agent.get_safety_performance()
    print("\nüìä Safety Performance:")
    action_mod_rate = performance['action_modification_rate']
    avg_safety_score = performance['avg_safety_score']
    conservative_mode = performance['conservative_mode']

    print(f"   Action modification rate: {action_mod_rate:.2%}")
    print(f"   Average safety score: {avg_safety_score:.3f}")
    print(f"   Conservative mode: {conservative_mode}")


async def demonstrate_explainable_rl() -> None:
    """Demonstrate explainable reinforcement learning."""
    print("\nüîç Demonstrating Explainable RL")
    print("=" * 60)

    # Initialize components
    model = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        api_key=os.getenv("ANTHROPIC_API_KEY"),
    )
    db = MemoryDatabase("explainable_rl_demo.db")
    reward_system = RewardSystem(db)

    # Create base agent
    base_agent = DQNAgent(
        name="base_dqn",
        model=model,
        db=db,
        reward_system=reward_system,
        state_dim=16,
        action_dim=4,
    )

    # Define meaningful feature names
    feature_names = [
        "user_satisfaction", "task_complexity", "resource_availability",
        "time_pressure", "data_quality", "system_load", "user_expertise",
        "task_priority", "context_relevance", "historical_success",
        "risk_level", "confidence_score", "collaboration_need",
        "creativity_required", "analysis_depth", "response_urgency"
    ]

    # Create explainable RL agent
    explainable_agent = await create_explainable_rl_agent(
        model=model,
        db=db,
        base_agent=base_agent,
        feature_names=feature_names,
        explanation_methods=["gradient", "permutation"],
    )

    print("‚úÖ Created explainable RL agent with:")
    print(f"   - {len(feature_names)} meaningful features")
    print("   - Gradient and permutation importance methods")
    print("   - Natural language explanation generation")
    print("   - Risk assessment capabilities")

    # Demonstrate explainable decision making
    print("\nüß† Generating explainable decisions...")

    decision_scenarios = [
        {
            "description": "Data analysis request",
            "context": {
                "request": "Analyze sales data for trends",
                "urgent": False
            },
            # High satisfaction, complexity, resources, low pressure
            "state_bias": [0.8, 0.6, 0.7, 0.3],
        },
        {
            "description": "Urgent creative task",
            "context": {
                "request": "Create marketing campaign urgently",
                "urgent": True
            },
            # Medium satisfaction, high complexity, low resources, high pressure
            "state_bias": [0.5, 0.9, 0.4, 0.9],
        },
        {
            "description": "Simple information lookup",
            "context": {
                "request": "Find contact information",
                "urgent": False
            },
            # High satisfaction, low complexity, good resources, no pressure
            "state_bias": [0.9, 0.2, 0.8, 0.1],
        },
    ]

    for i, scenario in enumerate(decision_scenarios):
        print(f"\nüìã Scenario {i+1}: {scenario['description']}")

        # Generate biased state to make explanations more meaningful
        state = np.random.randn(16).astype(np.float32)
        for j, bias in enumerate(scenario['state_bias']):
            if j < len(state):
                state[j] = bias + np.random.normal(0, 0.1)

        # Get action with explanation
        context = scenario["context"]
        action, explanation = await explainable_agent.select_action_with_explanation(
            state, context, training=True
        )

        print(f"   üéØ Selected action: {action}")
        print(f"   üéØ Confidence: {explanation.confidence:.1%}")
        print(f"   üí≠ Reasoning: {explanation.reasoning}")

        # Show top contributing factors
        top_factors = sorted(
            explanation.contributing_factors.items(),
            key=lambda x: abs(x[1]),
            reverse=True
        )[:3]

        print("   üîç Top factors:")
        for factor, importance in top_factors:
            print(f"     - {factor}: {importance:.3f}")

        # Show risk assessment
        risk = explanation.risk_assessment.get("overall", 0.0)
        print(f"   ‚ö†Ô∏è Risk level: {risk:.1%}")

        # Show alternatives
        if explanation.alternative_actions:
            alt = explanation.alternative_actions[0]
            action_num = alt['action']
            q_value = alt['q_value']
            print(f"   üîÑ Best alternative: Action {action_num} "
                  f"(Q-value: {q_value:.3f})")

    # Get explanation statistics
    stats = explainable_agent.get_explanation_statistics()
    print("\nüìä Explanation Statistics:")
    print(f"   Total explanations: {stats['total_explanations']}")
    print(f"   Average confidence: {stats['avg_confidence']:.1%}")
    print(f"   Average risk: {stats['avg_risk']:.1%}")
    print("   Top important features:")
    for feature, importance in stats['top_important_features'][:3]:
        print(f"     - {feature}: {importance:.3f}")


async def demonstrate_integrated_system() -> None:
    """Demonstrate integrated advanced RL system."""
    print("\nüöÄ Demonstrating Integrated Advanced RL System")
    print("=" * 60)

    # This would combine all the advanced features in a real scenario
    print("üîó Integration capabilities:")
    print("   ‚úÖ Distributed training with multiple workers")
    print("   ‚úÖ Automated hyperparameter optimization")
    print("   ‚úÖ Safety constraints and risk management")
    print("   ‚úÖ Explainable decision making")
    print("   ‚úÖ Meta-learning for fast adaptation")
    print("   ‚úÖ Multi-agent coordination")
    print("   ‚úÖ Curriculum learning progression")
    print("   ‚úÖ Advanced memory systems")

    print("\nüéØ Real-world applications:")
    print("   ‚Ä¢ Autonomous customer service systems")
    print("   ‚Ä¢ Intelligent resource management")
    print("   ‚Ä¢ Adaptive content generation")
    print("   ‚Ä¢ Risk-aware decision support")
    print("   ‚Ä¢ Explainable AI assistants")

    print("\nüîÆ Future enhancements:")
    print("   ‚Ä¢ Federated learning across organizations")
    print("   ‚Ä¢ Causal reasoning integration")
    print("   ‚Ä¢ Human-in-the-loop optimization")
    print("   ‚Ä¢ Real-time safety monitoring")
    print("   ‚Ä¢ Advanced explanation interfaces")


async def main() -> None:
    """Run complete advanced RL demonstration with Phase 3 optimizations."""
    print("üöÄ Complete Advanced Reinforcement Learning Demonstration")
    print("üîß Now with Phase 3 Performance Optimizations!")
    print("=" * 80)

    # Initialize global memory monitoring
    from src.utils.memory_monitor import get_global_monitor
    monitor = get_global_monitor(auto_start=True)

    with MemoryContext("complete_rl_demo", threshold_mb=10.0) as total_ctx:
        log_memory_usage("Starting complete RL demonstration")

        try:
            # Import required libraries using lazy loading
            from src.utils.lazy_imports import get_loaded_modules

            loaded_modules = len(get_loaded_modules())
            print(f"üìä Lazy loading status: {loaded_modules} modules loaded")

            # Run demonstrations with memory tracking
            await demonstrate_distributed_rl()
            log_memory_usage("After distributed RL demo")

            await demonstrate_hyperparameter_optimization()
            log_memory_usage("After hyperparameter optimization demo")

            await demonstrate_safe_rl()
            log_memory_usage("After safe RL demo")

            await demonstrate_explainable_rl()
            log_memory_usage("After explainable RL demo")

            await demonstrate_integrated_system()
            log_memory_usage("After integrated system demo")

            print("\nüéâ Complete advanced RL demonstration finished!")
            print(f"üíæ Total memory usage: {total_ctx.memory_delta:.2f}MB")

            # Get memory optimization report
            from src.utils.lazy_imports import get_memory_report
            lazy_report = get_memory_report()
            total_loaded = lazy_report['total_loaded']
            total_registered = lazy_report['total_registered']
            print(f"üìà Lazy loading efficiency: {total_loaded}/"
                  f"{total_registered} modules loaded")

            # Get global memory statistics
            memory_stats = monitor.get_summary_report()
            peak_memory = memory_stats['monitoring_stats']['peak_memory_mb']
            print(f"üß† Peak memory usage: {peak_memory:.2f}MB")

            print("\nüìã Summary of demonstrated capabilities:")
            print("   ‚úÖ Distributed RL - Scalable training across multiple workers")
            print("   ‚úÖ Hyperparameter Optimization - Automated tuning for "
                  "best performance")
            print("   ‚úÖ Safe RL - Constraint satisfaction and risk management")
            print("   ‚úÖ Explainable RL - Interpretable AI decisions with "
                  "natural language")
            print("   ‚úÖ Integration - All features working together "
                  "seamlessly")
            print("   ‚úÖ Phase 3 Optimizations - Memory efficiency and "
                  "performance")

            print("\nüèÜ Your RL system now includes:")
            print("   ‚Ä¢ State-of-the-art deep RL algorithms")
            print("   ‚Ä¢ Advanced training techniques")
            print("   ‚Ä¢ Production-ready safety features")
            print("   ‚Ä¢ Human-interpretable explanations")
            print("   ‚Ä¢ Scalable distributed architecture")
            print("   ‚Ä¢ Memory-optimized operation")
            print("   ‚Ä¢ Async database operations")
            print("   ‚Ä¢ Lazy loading for faster startup")
            print("   ‚Ä¢ Bounded collections for memory management")
            print("   ‚Ä¢ Dependency injection for clean architecture")

        except ImportError as e:
            print(f"‚ùå Missing dependency: {e}")
            print("üí° Please install required packages:")
            packages = "torch optuna sentence-transformers aiosqlite psutil"
            print(f"   pip install {packages}")
        except Exception as e:
            print(f"‚ùå Error during demonstration: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # Cleanup and final memory report
            monitor.stop_monitoring()
            print(f"\nüèÅ Final memory delta: {total_ctx.memory_delta:.2f}MB")


if __name__ == "__main__":
    asyncio.run(main())
